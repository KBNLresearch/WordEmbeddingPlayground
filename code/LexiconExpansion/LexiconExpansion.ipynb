{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lexicon Expansion\n",
    "\n",
    "## Traveling through word vector spaces\n",
    "\n",
    "This notebook provides tools to explore and interact with word embedding models. It allows you to create a lexicon (a list of conceptually related words) by traversing a vector space.\n",
    "The notebooks provides three methods for expansion:\n",
    "- Unidirectional expansion\n",
    "- Contrastive expansion\n",
    "- Active-learning based expansion\n",
    "\n",
    "See documentation below and [README](https://github.com/kasparvonbeelen/WordEmbeddingPlayground/tree/master/code/LexiconExpansion/README.md) for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In /home/kaspar/.local/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The text.latex.preview rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/kaspar/.local/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The mathtext.fallback_to_cm rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/kaspar/.local/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: Support for setting the 'mathtext.fallback_to_cm' rcParam is deprecated since 3.3 and will be removed two minor releases later; use 'mathtext.fallback : 'cm' instead.\n",
      "In /home/kaspar/.local/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The validate_bool_maybe_none function was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/kaspar/.local/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The savefig.jpeg_quality rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/kaspar/.local/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The keymap.all_axes rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/kaspar/.local/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The animation.avconv_path rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/kaspar/.local/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The animation.avconv_args rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "from modAL.uncertainty import entropy_sampling,margin_sampling, uncertainty_sampling\n",
    "from modAL.models import ActiveLearner\n",
    "from sklearn.svm import SVC\n",
    "from scipy.spatial.distance import cosine, euclidean\n",
    "from sklearn.model_selection import train_test_split\n",
    "from utils import *\n",
    "from sklearn.metrics import f1_score\n",
    "from collections import defaultdict\n",
    "import logging\n",
    "import sys\n",
    "import pickle\n",
    "import random\n",
    "import requests\n",
    "import datetime\n",
    "import pprint\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "from ipyannotate import annotate\n",
    "from ipyannotate.buttons import (\n",
    "    ValueButton as Button,\n",
    "    NextButton as Next,\n",
    "    BackButton as Back\n",
    ")\n",
    "\n",
    "def show_html(word,verbose='link'):\n",
    "    \n",
    "    link_template = '<a size=\"5\" color=\"black\" target=\"_blank\" style=\"font-family:courier\" href=\"{}\">{}</a>'\n",
    "    url = 'https://en.wiktionary.org/wiki/{}'.format(word)\n",
    "    wiki_url = 'https://en.wikipedia.org/w/index.php?sort=relevance&search={}'\n",
    "    \n",
    "    if verbose=='insert':\n",
    "        response = requests.get(url)\n",
    "        description = response.content.decode(\"utf-8\")\n",
    "    elif verbose=='link':\n",
    "        \n",
    "        wiktionary = link_template.format(url,\"Wiktionary\")\n",
    "        wikipedia = link_template.format(wiki_url.format('+'.join(word.split('-'))),\"Wikipedia\")\n",
    "        description = f\"{wiktionary}&nbsp;&nbsp;{wikipedia}\"\n",
    "    else:\n",
    "        description = ''\n",
    "        \n",
    "    return display(HTML('</br><font size=\"6\" color=\"black\" style=\"font-family:georgia;\">\"{0}\"</font></br></br>{1}'.format(word,description)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model\n",
    "\n",
    "Change path variable below to load a specific Word2Vec model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-10-28 09:00:33,266 : INFO : loading Word2Vec object from /kbdata/Processed/Models/1890-1910-Katholiek.w2v.model\n",
      "2020-10-28 09:00:33,961 : INFO : loading wv recursively from /kbdata/Processed/Models/1890-1910-Katholiek.w2v.model.wv.* with mmap=None\n",
      "2020-10-28 09:00:33,962 : INFO : loading vectors from /kbdata/Processed/Models/1890-1910-Katholiek.w2v.model.wv.vectors.npy with mmap=None\n",
      "2020-10-28 09:00:34,091 : INFO : setting ignored attribute vectors_norm to None\n",
      "2020-10-28 09:00:34,093 : INFO : loading vocabulary recursively from /kbdata/Processed/Models/1890-1910-Katholiek.w2v.model.vocabulary.* with mmap=None\n",
      "2020-10-28 09:00:34,093 : INFO : loading trainables recursively from /kbdata/Processed/Models/1890-1910-Katholiek.w2v.model.trainables.* with mmap=None\n",
      "2020-10-28 09:00:34,094 : INFO : loading syn1neg from /kbdata/Processed/Models/1890-1910-Katholiek.w2v.model.trainables.syn1neg.npy with mmap=None\n",
      "2020-10-28 09:00:34,222 : INFO : setting ignored attribute cum_table to None\n",
      "2020-10-28 09:00:34,223 : INFO : loaded /kbdata/Processed/Models/1890-1910-Katholiek.w2v.model\n"
     ]
    }
   ],
   "source": [
    "# for large models it works only with numpy 1.17.0\n",
    "# https://www.pythonanywhere.com/forums/topic/14613/\n",
    "# pip3 install numpy==1.17.0\n",
    "path = \"/kbdata/Processed/Models/1890-1910-Katholiek.w2v.model\"\n",
    "model = Word2Vec.load(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Unidirectional Lexicon Expansion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Select Seed Words\n",
    "\n",
    "Select the seed words, these will provide the starting point of the expansion. More precisely, the script below will average the vector representations of the seed words and save it as `core_init` the vector whose neighbourhood we will explore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "core = {'vrouw','moeder'}\n",
    "\n",
    "core_init = average_vector(core.copy(),model)\n",
    "\n",
    "seen = core.copy()\n",
    "peripheral = set()\n",
    "    \n",
    "rounds = 0\n",
    "log = defaultdict(dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Annotation Cycle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 Select Sampling Strategy\n",
    "<img src='img/average_vector_st_2.png' width=\"500\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lexicon Expansion provides a handful of functions to explore vector spaces and create word lists that track some kind of underlying concepts. Given a word list `L` at time `t`, to expand this list, we average the vector representations of the words in `L` (i.e. combine them in one vector) and use it a a query vector `q_v`. The functions listed below, allow you to navigate the area around `q_v`: \n",
    "\n",
    "Select the sampling procedure, this effect the procedure for navigating the vector space. The options here are:\n",
    "\n",
    "- `average`: samples the word closest to `q_v`; \n",
    "- `query_tokens`: add selected tokens to `q_v` and sample the closest neighbours;\n",
    "- `entropy`: sample neighbours of `q_v`  based on the entropy scores entropy(`q_v`,`neighbour_n`);\n",
    "- `distance`: select neighbours with the highest distance to `q_v` (according to some distance metric such as cosine similarity) to avg.\n",
    "\n",
    "Uncomment the code below, if you want to change the sampling procedure.\n",
    "\n",
    "\n",
    "The figure above, visualizes the `average` sampling strategy. We start with two vectors (\"machine\" and \"engine\"), combine them into one vector by averaging the activations, and then query the neighbourhood of this averaged vector, add \"spinning-jenny\" and \"power-loom\", after which we can repeat the whole procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'method': <function utils.average_all(words, model)>}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampling_procedure = sampling_options['average']\n",
    "sampling_procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampling_procedure = sampling_options['query_tokens']\n",
    "# sampling_procedure['args']['tokens'] = list(core)\n",
    "# sampling_procedure['args']['merge'] = False\n",
    "# sampling_procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampling_procedure =  sampling_options['entropy']\n",
    "# sampling_procedure[\"args\"]['init_vec'] = core_init\n",
    "# sampling_procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2  Annotate Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By running the cell below, you can annotate words. `Core` words will be used to update the query vector `q_v`, `Periphal` words will be saved but not used for expansion. You can use it, for example, to keep track of words with OCR errors, but don't allow them to contaminate the creation the word list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-10-27 14:22:00,753 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using \"average_all\" as sample method.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3598d4594a64236b542684b850aeefa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Annotation(canvas=OutputCanvas(), progress=Progress(atoms=[<ipyannotate.progress.Atom object at 0x7fd06318c390…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "log = update_log(log,rounds,seen,core,peripheral,sampling_procedure)\n",
    "\n",
    "neighbours = expand_lexicon(core,model,**sampling_procedure)\n",
    "neighbours = topn_new(neighbours,seen,topn=5)                     \n",
    "\n",
    "buttons = [Button('Core',color='green'),\n",
    "           Button('Peripheral',color='blue'),\n",
    "           Button('Ignore',color='red'), Back(), Next()]   \n",
    "\n",
    "annotations = annotate(list(neighbours.keys()), buttons=buttons, display = show_html)\n",
    "annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.3 Add Annotations to Lexicon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After each round of annotations, add the selected words (`Core` and `Peripheral`) to the lexicon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "core.update([t.output for t in annotations.tasks if t.value=='Core'])\n",
    "peripheral.update([t.output for t in annotations.tasks if t.value=='Peripheral'])\n",
    "seen.update([t.output for t in annotations.tasks])\n",
    "rounds+=1\n",
    "log = update_log(log,rounds,seen,core,peripheral,sampling_procedure)\n",
    "print('Core Lexicon contains {0} tokens at stage {1}.\\n'.format(len(core), rounds))  \n",
    "print(', '.join(core))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Inspect Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At each stage of the annotations, you can inspects your lexicon visually. Running the cell below, will project all words on a 2D plane. The red dots represent the `Core` vocabulary, whereas the green dots show you the surrounding tokens not yet in the lexicon. It could give you an idea of the words you are missing, and the next destination for word vector space travel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_travel_distance(log,model,core_init,method=np.mean)\n",
    "plot_2d(log,model,figsize=(10,10),include_neighbours=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. Save Annotations\n",
    "\n",
    "After multiple round of annotation you can save the log of your work for later analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = datetime.datetime.now()\n",
    "with open('logged_annotations_{}.pickle'.format(datetime.datetime.now().strftime(\"%Y-%m-%d_%H:%M:%S\")),'wb') as out_pickle:\n",
    "    pickle.dump(log,out_pickle)\n",
    "print('Annotations saved!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Bi-Directional Lexicon Expansion\n",
    "\n",
    "<img src='img/projection.png' width=\"500\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Select Seed Words\n",
    "\n",
    "Select the two opposite ends for contrastive lexicons you want to create. The `Core` and `Anitode` serves as two contrastive \"feminine\" and \"masculine\" word lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "core = {'vrouw','vrouwen'}\n",
    "antipode = {'man','mannen'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "periferal=None\n",
    "seen=None\n",
    "\n",
    "seen = core.copy().union(antipode)\n",
    "periferal = set()\n",
    "    \n",
    "rounds = 0\n",
    "log = defaultdict(dict)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Annotation Cycle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1  Annotate Words\n",
    "\n",
    "Assign sampled words to a either `Core` or `Antipode` lexicon. This section consist of two rounds of annotation, the first one focussed on `Core` words, the second one on `Antipode` words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore the `Core` end of the spectrum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log[rounds]['timestamp'] = datetime.datetime.now()           \n",
    "log[rounds]['seen'] = seen.copy(); log[rounds]['core'] = core.copy()\n",
    "log[rounds]['periferal'] = periferal.copy(); log[rounds]['antipode'] = antipode.copy()\n",
    "\n",
    "neighbours = contrastive_expansion(core,model,antipode,direction='core')\n",
    "neighbours = topn_new(neighbours,seen,topn=5)    \n",
    "\n",
    "buttons = [Button('Core',color='green'),Button('Antipode',color='green'),\n",
    "           Button('Peripheral',color='blue'),Button('Ignore',color='red'),\n",
    "           Back(), Next()]\n",
    "\n",
    "annotations_core = annotate(list(neighbours.keys()), buttons=buttons, display = show_html)\n",
    "annotations_core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore the `Antipode` end of the spectrum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbours = contrastive_expansion(core,model,antipode,direction='antipode')\n",
    "neighbours = topn_new(neighbours,seen,reverse=False,topn=5)  \n",
    "\n",
    "buttons = [Button('Antipode',color='green'),Button('Core',color='green'),\n",
    "           Button('Peripheral',color='blue'),Button('Ignore',color='red'),\n",
    "           Back(), Next()]\n",
    "\n",
    "annotations_antipode = annotate(list(neighbours.keys()), buttons=buttons, display = show_html)\n",
    "annotations_antipode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2. Add Annotations to Lexicons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run these cells to update the lexicon, and inspects to words harvested so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations = annotations_antipode.tasks + annotations_core.tasks\n",
    "core.update([t.output for t in annotations if t.value=='Core'])\n",
    "antipode.update([t.output for t in annotations if t.value=='Antipode'])\n",
    "periferal.update([t.output for t in annotations if t.value=='Peripheral'])\n",
    "seen.update([t.output for t in annotations])\n",
    "rounds+=1\n",
    "print('Core-lexicon at stage {0} contains {1} words.\\nSize of Antipode-lexicon is {2} words'.format(rounds,len(core),len(antipode)))      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Core Lexicon at stage {}.\\n'.format(rounds))\n",
    "print(', '.join(core))\n",
    "print('\\n')\n",
    "print('Antipode Lexicon at stage {}.\\n'.format(rounds))\n",
    "print(', '.join(antipode))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. Save Annotations\n",
    "\n",
    "Save the output for later analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = datetime.datetime.now()\n",
    "with open('logged_annotations_{}.pickle'.format(datetime.datetime.now().strftime(\"%Y-%m-%d_%H:%M:%S\")),'wb') as out_pickle:\n",
    "    pickle.dump(log,out_pickle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Active Leaning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Define seed words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1 Collect annotated seed words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 'machine'; neighbourhood = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "machine_neighbours = np.array([w for w,v in model.wv.most_similar(seed,topn=neighbourhood)])\n",
    "seed_idx = np.random.choice(range(len(machine_neighbours)), size=50, replace=False)\n",
    "seed_words = machine_neighbours[seed_idx]\n",
    "print(seed_words)\n",
    "buttons = [Button('Core',color='green'),Button('Antipode',color='red'),\n",
    "           Button('Ignore',color='blue'),Back(), Next()]\n",
    "\n",
    "annotations_init = annotate(seed_words, buttons=buttons, display = show_html)\n",
    "annotations_init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2 Initialize learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "core_init = [t.output for t in annotations_init.tasks if t.value=='Core']\n",
    "antipode_init = [t.output for t in annotations_init.tasks if t.value=='Antipode']\n",
    "print(len(core_init),len(antipode_init))\n",
    "seed_words_annotated = antipode_init + core_init\n",
    "X_initial = np.array([model.wv[w] for w in seed_words_annotated])\n",
    "y_initial = np.array([0]*len(antipode_init) + [1]*len(core_init))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([model.wv[w] for w,v in model.wv.most_similar(seed,topn=1000)])\n",
    "words = np.array([w for w,v in model.wv.most_similar(seed,topn=1000)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_training, X_testing, y_training, y_testing = train_test_split(X_initial, y_initial, test_size=0.50, random_state=0) \n",
    "initial_idx = np.array([i for i,w in enumerate(words) if w in seed_words_annotated])\n",
    "X_pool,y_pool = np.delete(X, initial_idx, axis=0),np.delete(words, initial_idx, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing the learner\n",
    "learner = ActiveLearner(\n",
    "    estimator=SVC(probability=True,kernel='linear'), # ,class_weight='balanced',C=10\n",
    "    query_strategy=uncertainty_sampling,\n",
    "    X_training=X_training, y_training=y_training\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Annotation cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_idx, query_inst = learner.query(X_pool,10)\n",
    "to_label = [y_pool[qx] for qx in query_idx]#{words[qx]:[qx,list(q_inst)] for qx,q_inst in zip(query_idx, query_inst)}\n",
    "\n",
    "buttons = [Button('Core',color='green'),Button('Antipode',color='red'),\n",
    "           Button('Ignore',color='blue'),Back(), Next()]\n",
    "\n",
    "annotations = annotate(to_label, buttons=buttons, display=show_html)\n",
    "annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pool,y_pool = np.delete(X_pool, query_idx, axis=0),np.delete(y_pool, query_idx, axis=0)\n",
    "y_new = [{'Core':1, 'Antipode': 0}.get(a.value,0) for a in annotations.tasks]\n",
    "learner.teach(query_inst,y_new)  # \n",
    "y_pred = learner.predict(X_testing)\n",
    "scores.append(f1_score(y_pred,y_testing))\n",
    "print('Done updating model. Go to previous cell to annotate other examples.')\n",
    "pd.Series(scores).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Print results of trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs= dict(zip(words,learner.predict_proba(X)[:,1]))\n",
    "machine_words = sorted(probs.items(),key = lambda x : x[1], reverse=True)[:100]\n",
    "#print('\\n'.join([f'{e[0]},{round(e[1],2)}' for e in machine_words]))\n",
    "print('\\n'.join(['{0: <20}{1}'.format(e[0],round(e[1],2)) for e in machine_words]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
