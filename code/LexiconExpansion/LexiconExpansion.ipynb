{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In /home/kaspar/.local/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The text.latex.preview rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/kaspar/.local/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The mathtext.fallback_to_cm rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/kaspar/.local/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: Support for setting the 'mathtext.fallback_to_cm' rcParam is deprecated since 3.3 and will be removed two minor releases later; use 'mathtext.fallback : 'cm' instead.\n",
      "In /home/kaspar/.local/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The validate_bool_maybe_none function was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/kaspar/.local/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The savefig.jpeg_quality rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/kaspar/.local/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The keymap.all_axes rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/kaspar/.local/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The animation.avconv_path rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/kaspar/.local/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The animation.avconv_args rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "from modAL.uncertainty import entropy_sampling,margin_sampling, uncertainty_sampling\n",
    "from modAL.models import ActiveLearner\n",
    "from sklearn.svm import SVC\n",
    "from scipy.spatial.distance import cosine, euclidean\n",
    "from sklearn.model_selection import train_test_split\n",
    "from utils import *\n",
    "from sklearn.metrics import f1_score\n",
    "from collections import defaultdict\n",
    "import logging\n",
    "import sys\n",
    "import pickle\n",
    "import random\n",
    "import requests\n",
    "import datetime\n",
    "import pprint\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "from ipyannotate import annotate\n",
    "from ipyannotate.buttons import (\n",
    "    ValueButton as Button,\n",
    "    NextButton as Next,\n",
    "    BackButton as Back\n",
    ")\n",
    "\n",
    "def show_html(word,verbose='link'):\n",
    "    \n",
    "    link_template = '<a size=\"5\" color=\"black\" target=\"_blank\" style=\"font-family:courier\" href=\"{}\">{}</a>'\n",
    "    url = 'https://en.wiktionary.org/wiki/{}'.format(word)\n",
    "    wiki_url = 'https://en.wikipedia.org/w/index.php?sort=relevance&search={}'\n",
    "    \n",
    "    if verbose=='insert':\n",
    "        response = requests.get(url)\n",
    "        description = response.content.decode(\"utf-8\")\n",
    "    elif verbose=='link':\n",
    "        \n",
    "        wiktionary = link_template.format(url,\"Wiktionary\")\n",
    "        wikipedia = link_template.format(wiki_url.format('+'.join(word.split('-'))),\"Wikipedia\")\n",
    "        description = f\"{wiktionary}&nbsp;&nbsp;{wikipedia}\"\n",
    "    else:\n",
    "        description = ''\n",
    "        \n",
    "    return display(HTML('</br><font size=\"6\" color=\"black\" style=\"font-family:georgia;\">\"{0}\"</font></br></br>{1}'.format(word,description)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model\n",
    "\n",
    "Change path variable below to load a specific Word2Vec model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-10-27 14:21:52,606 : INFO : loading Word2Vec object from /kbdata/Processed/Models/1890-1910-Katholiek.w2v.model\n",
      "2020-10-27 14:21:53,278 : INFO : loading wv recursively from /kbdata/Processed/Models/1890-1910-Katholiek.w2v.model.wv.* with mmap=None\n",
      "2020-10-27 14:21:53,279 : INFO : loading vectors from /kbdata/Processed/Models/1890-1910-Katholiek.w2v.model.wv.vectors.npy with mmap=None\n",
      "2020-10-27 14:21:53,406 : INFO : setting ignored attribute vectors_norm to None\n",
      "2020-10-27 14:21:53,407 : INFO : loading vocabulary recursively from /kbdata/Processed/Models/1890-1910-Katholiek.w2v.model.vocabulary.* with mmap=None\n",
      "2020-10-27 14:21:53,408 : INFO : loading trainables recursively from /kbdata/Processed/Models/1890-1910-Katholiek.w2v.model.trainables.* with mmap=None\n",
      "2020-10-27 14:21:53,408 : INFO : loading syn1neg from /kbdata/Processed/Models/1890-1910-Katholiek.w2v.model.trainables.syn1neg.npy with mmap=None\n",
      "2020-10-27 14:21:53,541 : INFO : setting ignored attribute cum_table to None\n",
      "2020-10-27 14:21:53,541 : INFO : loaded /kbdata/Processed/Models/1890-1910-Katholiek.w2v.model\n"
     ]
    }
   ],
   "source": [
    "# for large models it works only with numpy 1.17.0\n",
    "# https://www.pythonanywhere.com/forums/topic/14613/\n",
    "# pip3 install numpy==1.17.0\n",
    "path = \"/kbdata/Processed/Models/1890-1910-Katholiek.w2v.model\"\n",
    "model = Word2Vec.load(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lexicon Expansion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Unidirectional Lexicon Expansion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Select Seed Words\n",
    "\n",
    "Select the seed words, these will provide the starting point of the expansion. More precisely, the script below will average the vector representation of the seed words and save it as `core_init` the vector whose neighbourhood we explore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "core = {'vrouw','moeder'}\n",
    "\n",
    "core_init = average_vector(core.copy(),model)\n",
    "\n",
    "seen = core.copy()\n",
    "peripheral = set()\n",
    "    \n",
    "rounds = 0\n",
    "log = defaultdict(dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Annotation Cycle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 Select Sampling Strategy\n",
    "\n",
    "Select the sampling procedure, this effect the procedure for navigating the vector space. The options here are:\n",
    "\n",
    "- `average`: computes the average vector `avg_v` of words the Lexicon `L` at time `t`; in iteration `t+1` it samples the word closest to `avg_v`; \n",
    "- `query_tokens`: query the area around a selected list of tokens, or add these tokens to the existing dictionary\n",
    "- `entropy`: given a average vector `avg_v`, select neighbouring words whose vector representation have the highest entropy compored to `avg_v`;\n",
    "- `distance`: given a average vector `avg_v`, select neighbourwords that are furthest away (according to some distance metric such as cosine similarity) to avg.\n",
    "\n",
    "Uncomment the code below, if you want to change the sampling procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'method': <function utils.average_all(words, model)>}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampling_procedure = sampling_options['average']\n",
    "sampling_procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampling_procedure = sampling_options['query_tokens']\n",
    "# sampling_procedure['args']['tokens'] = list(core)\n",
    "# sampling_procedure['args']['merge'] = False\n",
    "# sampling_procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampling_procedure =  sampling_options['entropy']\n",
    "# sampling_procedure[\"args\"]['init_vec'] = core_init\n",
    "# sampling_procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2  Annotate Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-10-27 14:22:00,753 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using \"average_all\" as sample method.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3598d4594a64236b542684b850aeefa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Annotation(canvas=OutputCanvas(), progress=Progress(atoms=[<ipyannotate.progress.Atom object at 0x7fd06318c390…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "log = update_log(log,rounds,seen,core,peripheral,sampling_procedure)\n",
    "\n",
    "neighbours = expand_lexicon(core,model,**sampling_procedure)\n",
    "neighbours = topn_new(neighbours,seen,topn=5)                     \n",
    "\n",
    "buttons = [Button('Core',color='green'),\n",
    "           Button('Peripheral',color='blue'),\n",
    "           Button('Ignore',color='red'), Back(), Next()]   \n",
    "\n",
    "annotations = annotate(list(neighbours.keys()), buttons=buttons, display = show_html)\n",
    "annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.3 Add Annotations to Lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "core.update([t.output for t in annotations.tasks if t.value=='Core'])\n",
    "peripheral.update([t.output for t in annotations.tasks if t.value=='Peripheral'])\n",
    "seen.update([t.output for t in annotations.tasks])\n",
    "rounds+=1\n",
    "log = update_log(log,rounds,seen,core,peripheral,sampling_procedure)\n",
    "print('Core Lexicon contains {0} tokens at stage {1}.\\n'.format(len(core), rounds))  \n",
    "print(', '.join(core))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Inspect Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_travel_distance(log,model,core_init,method=np.mean)\n",
    "plot_2d(log,model,figsize=(10,10),include_neighbours=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. Save Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = datetime.datetime.now()\n",
    "with open('logged_annotations_{}.pickle'.format(datetime.datetime.now().strftime(\"%Y-%m-%d_%H:%M:%S\")),'wb') as out_pickle:\n",
    "    pickle.dump(log,out_pickle)\n",
    "print('Annotations saved!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Bi-Directional Lexicon Expansion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Select Seed Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "core = {'machine','machines','machinery'}\n",
    "antipode = {'human','humans','humanity'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "periferal=None\n",
    "seen=None\n",
    "\n",
    "seen = core.copy().union(antipode)\n",
    "periferal = set()\n",
    "    \n",
    "rounds = 0\n",
    "log = defaultdict(dict)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Annotation Cycle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1  Annotate Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log[rounds]['timestamp'] = datetime.datetime.now()           \n",
    "log[rounds]['seen'] = seen.copy(); log[rounds]['core'] = core.copy()\n",
    "log[rounds]['periferal'] = periferal.copy(); log[rounds]['antipode'] = antipode.copy()\n",
    "\n",
    "neighbours = contrastive_expansion(core,model,antipode,direction='core')\n",
    "neighbours = topn_new(neighbours,seen,topn=5)    \n",
    "\n",
    "buttons = [Button('Core',color='green'),Button('Antipode',color='green'),\n",
    "           Button('Peripheral',color='blue'),Button('Ignore',color='red'),\n",
    "           Back(), Next()]\n",
    "\n",
    "annotations_core = annotate(list(neighbours.keys()), buttons=buttons, display = show_html)\n",
    "annotations_core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbours = contrastive_expansion(core,model,antipode,direction='antipode')\n",
    "neighbours = topn_new(neighbours,seen,reverse=False,topn=5)  \n",
    "\n",
    "buttons = [Button('Antipode',color='green'),Button('Core',color='green'),\n",
    "           Button('Peripheral',color='blue'),Button('Ignore',color='red'),\n",
    "           Back(), Next()]\n",
    "\n",
    "annotations_antipode = annotate(list(neighbours.keys()), buttons=buttons, display = show_html)\n",
    "annotations_antipode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2. Add Annotations to Lexicons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "annotations = annotations_antipode.tasks + annotations_core.tasks\n",
    "core.update([t.output for t in annotations if t.value=='Core'])\n",
    "antipode.update([t.output for t in annotations if t.value=='Antipode'])\n",
    "periferal.update([t.output for t in annotations if t.value=='Peripheral'])\n",
    "seen.update([t.output for t in annotations])\n",
    "rounds+=1\n",
    "print('Core-lexicon at stage {0} contains {1} words.\\nSize of Antipode-lexicon is {2} words'.format(rounds,len(core),len(antipode)))      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Core Lexicon at stage {}.\\n'.format(rounds))\n",
    "print(', '.join(core))\n",
    "print('\\n')\n",
    "print('Antipode Lexicon at stage {}.\\n'.format(rounds))\n",
    "print(', '.join(antipode))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. Save Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = datetime.datetime.now()\n",
    "with open('logged_annotations_{}.pickle'.format(datetime.datetime.now().strftime(\"%Y-%m-%d_%H:%M:%S\")),'wb') as out_pickle:\n",
    "    pickle.dump(log,out_pickle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Active Leaning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Define seed words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1 Collect annotated seed words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 'machine'; neighbourhood = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "machine_neighbours = np.array([w for w,v in model.wv.most_similar(seed,topn=neighbourhood)])\n",
    "seed_idx = np.random.choice(range(len(machine_neighbours)), size=50, replace=False)\n",
    "seed_words = machine_neighbours[seed_idx]\n",
    "print(seed_words)\n",
    "buttons = [Button('Core',color='green'),Button('Antipode',color='red'),\n",
    "           Button('Ignore',color='blue'),Back(), Next()]\n",
    "\n",
    "annotations_init = annotate(seed_words, buttons=buttons, display = show_html)\n",
    "annotations_init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2 Initialize learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "core_init = [t.output for t in annotations_init.tasks if t.value=='Core']\n",
    "antipode_init = [t.output for t in annotations_init.tasks if t.value=='Antipode']\n",
    "print(len(core_init),len(antipode_init))\n",
    "seed_words_annotated = antipode_init + core_init\n",
    "X_initial = np.array([model.wv[w] for w in seed_words_annotated])\n",
    "y_initial = np.array([0]*len(antipode_init) + [1]*len(core_init))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([model.wv[w] for w,v in model.wv.most_similar(seed,topn=1000)])\n",
    "words = np.array([w for w,v in model.wv.most_similar(seed,topn=1000)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_training, X_testing, y_training, y_testing = train_test_split(X_initial, y_initial, test_size=0.50, random_state=0) \n",
    "initial_idx = np.array([i for i,w in enumerate(words) if w in seed_words_annotated])\n",
    "X_pool,y_pool = np.delete(X, initial_idx, axis=0),np.delete(words, initial_idx, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing the learner\n",
    "learner = ActiveLearner(\n",
    "    estimator=SVC(probability=True,kernel='linear'), # ,class_weight='balanced',C=10\n",
    "    query_strategy=uncertainty_sampling,\n",
    "    X_training=X_training, y_training=y_training\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Annotation cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_idx, query_inst = learner.query(X_pool,10)\n",
    "to_label = [y_pool[qx] for qx in query_idx]#{words[qx]:[qx,list(q_inst)] for qx,q_inst in zip(query_idx, query_inst)}\n",
    "\n",
    "buttons = [Button('Core',color='green'),Button('Antipode',color='red'),\n",
    "           Button('Ignore',color='blue'),Back(), Next()]\n",
    "\n",
    "annotations = annotate(to_label, buttons=buttons, display=show_html)\n",
    "annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pool,y_pool = np.delete(X_pool, query_idx, axis=0),np.delete(y_pool, query_idx, axis=0)\n",
    "y_new = [{'Core':1, 'Antipode': 0}.get(a.value,0) for a in annotations.tasks]\n",
    "learner.teach(query_inst,y_new)  # \n",
    "y_pred = learner.predict(X_testing)\n",
    "scores.append(f1_score(y_pred,y_testing))\n",
    "print('Done updating model. Go to previous cell to annotate other examples.')\n",
    "pd.Series(scores).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Print results of trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs= dict(zip(words,learner.predict_proba(X)[:,1]))\n",
    "machine_words = sorted(probs.items(),key = lambda x : x[1], reverse=True)[:100]\n",
    "#print('\\n'.join([f'{e[0]},{round(e[1],2)}' for e in machine_words]))\n",
    "print('\\n'.join(['{0: <20}{1}'.format(e[0],round(e[1],2)) for e in machine_words]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
