**[onder constructie, niet publiceren]**
## Inleiding: hoe kranten 'mannen' en 'vrouwen' verbeelden

"Waarom meisjes glimlachen en jongens niet huilen" onderzoekt genderstereotypering in Nederlandse historische kranten. Het project maakt gebruik van computationele technieken om de stereotype weergave van "mannen" en "vrouwen" in de 19e en 20e eeuw te analyseren.

Om duidelijk te maken wat we met "stereotypen" of "vooroordelen" bedoelen, laten we een fragment uit het "Nieuwsblad van het Noorden" bekijken. Zoals andere kranten, poogde het Nieuwsblad hun lezers te onderwijzen met moraliserende gezegden. Op maandag 9 maart 1896 staat er “Door zachtheid regeert de vrouw, door kracht de man." Dergelijke overpeinzingen over het "correcte" gedrag van vrouwen en mannen waren wijdverbreid en gelijksoortige fragmenten keren vaak terug, zij het met kleine variaties: "Eene der schoonste deugden van de vrouw is zachtheid" schrijft de Rotterdamse Courant in 1896. Enkele jaren later, in 1912, resoneert deze zin in een passage gepubliceerd in Turbantie: "De meest gewenschte eigenschap de vrouw is zachtheid." Vrouwen afbeelden als "zacht" en mannen als "krachtig" is een flagrante vorm van stereotypering: een discursief proces bedoelen waarbij eigenschappen selectief worden toegeschreven aan sociale categorieën.

Een eenvoudige zoekopdracht in de Delpher Krantendatabase naar artikels die "vrouw" en "zachtheid" bevatten, levert meer dan 12.000 hits op—en suggereert dus dat zulke ideeën grotendeels gemeengoed waren, zeker tot aan het midden van de twintigste eeuw. Hoewel dergelijke zoekopdracht nuttige inzichten opleveren, blijft het moeilijk om (1) de verspreiding van dergelijke stereotypen te kwantificeren en (2) de intensiteit te meten waarmee gender-categorieën werden geassocieerd met bepaalde attributen (bijv. vrouwen met "zachtheid" en mannen met "kracht"). Traditionele historische methoden maakten gebruik van close reading om deze vraag te beantwoorden, maar een dergelijke benadering schaalt niet naar een corpus zo groot als de Delpher-database. Dit project gebruikt daarom “distant reading” om gendervooroordelen op een “macro” schaal in kaart te brengen; het beoogt zowel de inhoud als de verspreiding van genderstereotypen in Nederlandse kranten te meten.

## Onderzoekscontext: _bias_ en kunstmatige intelligentie

De ambities van dit project sluiten aan bij een bredere onderzoeksagenda die zich toelegt op de rol van _bias_* in kunstmatige intelligentie (KI). KI is snel opkomende technologie, die vele aspecten van het leven doordringt--van online winkelen tot verkiezingscampagnes. Onderzoekers maken zich daarom steeds meer zorgen over de sociale en politieke effecten van KI. Specifieke toepassingen, zoals gezichtsherkenning, zijn veelvuldig bekritiseerd omdat ze mensen van kleur discrimineren: de bestaande systemen werken goed voor het herkennen van gezichten van sociaal-economische machtige groepen (zoals blanke mannen) maar kunnen mensen met andere [huidtinten en gelaatsstructuur](https://www.ted.com/talks/joy_buolamwini_how_i_m_fighting_bias_in_algorithms?language=en) niet nauwkeurig lezen .

Het risico bestaat dus dat KI de ongelijkheden die de “offline” wereld beheersen, “online” repliceert. Hetzelfde geldt voor de reproductie van sociale vooroordelen. KI-applicaties zijn vaak gebaseerd op "Machine Learning", een techniek waarbij computers een bepaald gedrag "leren" aan de hand van een aantal voorbeelden (denk aan de spamfilter, waar een algoritme leert om inkomende e-mail als spam of niet-spam te herkennen, gegeven een aantal voorbeelden) . Nu wil het dat computer hier voorbeeld namen aan menselijk gedrag (en oordelen) en deze zo goed mogelijk pogen te repliceren. Problematisch is dat zij daarbij ook de menselijke vooroordelen en biases overnemen, zoals enkele artikels recent aangetoond.

Om dit in detail uit te leggen, laten we het voorbeeld van word embedding models (WEMs) nader bekijken. WEMs zijn een verzameling algoritmen die associaties leren tussen woorden op basis van hun gemeenschappelijk voorkomen (co-occurence) in een corpus. Een populair model met de naam Word2Vec stamt uit 2013. Het algoritme creëert een semantische ruimte (denk aan een driedimensionale vectorruimte, gedefinieerd door x-, y- en z-assen, zie figuur 1), en duwt woorden die vaak voorkomen (binnen een bepaalde context) dicht bij elkaar, terwijl het andere woorden verder weg schuift. (Ik maakt hier grote stappen, voor een degelijke introductie tot Word2Vec voor Digital Humanities lees deze [blogpost](http://bookworm.benschmidt.org/posts/2015-10-25-Word-Embeddings.html) van Ben Schmidt.

![Voorbeeld van een drie-dimensionale word embedding ruimte](./figures/figuur1.png)

Word2Vec genereert een ordening van woorden in een vectorruimte (dat wil zeggen dat het woorden in een “embed" in een vectorruimte), op basis waarvan onderzoekers vervolgens kunnen berekenen hoe vergelijkbaar (“dichtbij”) twee woorden zijn ("Kat" is dichter bij "Hond" dan bij "Steen", aangezien de eerste veel tekstuele contexten delen in tegenstelling tot de laatste).

Word2Vec is een krachtige tool voor veel KI-toepassingen (kijk bijvoorbeeld naar het aantal referenties in Google Scholar om jezelf te overtuigen). Maar met de populariteit kwam ook kritiek: een artikel van (Bolukbasi et al., 2016) toonde aan dat Word2Vec de neiging heeft om "onversneden seksistische" ("blatantly sexist") stereotypen te produceren, waarin bijvoorbeeld vrouwen worden geassocieerd met beroep als "verpleegkundige" en "receptionisten" en mannen met "meastro" of "filosoof".

Op deze bevinding volgde een stroom artikelen die sociale biases in modellen onderzochten. Terwijl (Bolukbasi et al., 2016) zich concentreerden op het 'debiasing' van de WEMs (om verspreiding van de stereotypen naar andere KI-applicaties te voorkomen), gebruikten andere artikelen deze bias een “signaal”: als een lens om de maatschappij bestuderen.  (Caliskan et al., 2017) demonstreerden rigoureus het bestaan van menselijke vooroordelen in WEM's, door stereotypen afgeleid van WEM's te vergelijken met die verkregen uit geaccepteerde psychologische tests (zoals de Implicit Association Test).

## Verwachte uitkomsten: bias als historisch signaal

Dit project breidt deze inzichten uit tot het domein van de geschiedenis, ervan uitgaande dat als woordinbedding getrainde hedendaagse gegevens hedendaagse vooroordelen repliceren, hetzelfde geldt voor het verleden.

(Garg et al., 2018) zorgden voor een meer rigoureuze validatie van deze intuïtie met het argument dat woordinbedding getraind op historische gegevens de veranderende verdeling van mannen en vrouwen in het personeel weerspiegelt, maar ook stereotiepe eigenschappen vastlegt. Door het bestuderen van de bijvoeglijke naamwoorden geassocieerd met Chinese namen, hebben de auteurs aangetoond dat de houding ten opzichte van Aziatische Amerikanen evolueerde van negatief (Chinees wordt gezien als "barbaars") naar grotendeels positief (Aziatische Amerikanen als de "model" minderheid). Met andere woorden, woordbedding stelt ons in staat om vertekening in de tijd te volgen. Hoewel we niet terug kunnen gaan naar het verleden, en vorige generaties kunnen onderwerpen aan psychologische tests (om de stereotypen die ze koesteren te achterhalen), lijken de resultaten van WEM's sterk op die welke zouden zijn verkregen met behulp van een tijdmachine.

In navolging van de krant (Garg et al. 2018) - en meer recentelijk (Wevers, 2019) - gebruiken we historische kranten om de gendervooroordelen van de afgelopen generaties te onderzoeken. Maar op welke manier zal dit onderzoek (en de verwachte resultaten) de huidige stand van de techniek verbeteren en andere onderzoeken helpen? Over het algemeen hopen we bij te dragen aan een meer verfijnd begrip van gendervooroordeel en de verspreiding ervan in Nederland.

- Ten eerste is het doel om een ​​eenvoudig hulpmiddel te maken waarmee onderzoekers de training van woordinbedding kunnen parametriseren, b.v. selecteer artikelen per jaar en specifieke krantenattributen (zoals bereik (nationaal of provinciaal), plaats van publicatie, politieke voorkeur en / of religieuze oriëntatie). Na het selecteren van de trainingsgegevens, kan de gebruiker kiezen tussen Word2Vec (2013) of het recentere FastText (2016) -algoritme om de woordinbedding te genereren. We zijn van plan te experimenteren met het verfijnen van contextuele modellen (zoals Bert). Alle modellen die zijn getraind als onderdeel van het onderzoek, zullen openbaar worden gemaakt.

- Ten tweede maken we het begrip "mannen" en "vrouwen" ingewikkelder: bestaande kranten gebruiken een relatief kleine (of bepaalde) reeks woorden om gendervooroordelen te berekenen. Ze berekenden hoe bepaalde attributen (bijv. "Vriendelijkheid") worden geassocieerd met een vrouwelijke / mannelijke naam of expliciet gesekse woorden zoals het voornaamwoord "zij". Voor een goed historisch onderzoek zijn we echter van plan eerst de hedendaagse terminologie (welke woorden door de historische krant werden gebruikt om mannen en vrouwen te beschrijven) onder de loep te nemen en te bekijken of de vertekening van toepassing was op alle mannelijke of vrouwelijke woorden, of het resultaat is van bijzonder sterk associaties met een bepaalde subcategorie (bijvoorbeeld moeders of meer bepaalde beroepen).

- Ten derde willen we, naast het monitoren van de verandering in bias op macroschaal (dat wil zeggen per decennium), begrijpen waar de bias eigenlijk vandaan komt (Brunet et al., 2019), dwz welke artikelen, kranten, acteurs of debatten zijn toegenomen op verminderde de bias? In die zin heeft ons onderzoek een fijnmazig begrip van de circulatie van vertekening in tijd en ruimte.

* We laten de term _bias_ onvertaald onwille van haar complexiteit.

## References

Bolukbasi, Tolga, Kai-Wei Chang, James Y. Zou, Venkatesh Saligrama, and Adam T. Kalai. "Man is to computer programmer as woman is to homemaker? debiasing word embeddings." In Advances in neural information processing systems, pp. 4349-4357. 2016.

Brunet, Marc-Etienne, Colleen Alkalay-Houlihan, Ashton Anderson, and Richard Zemel. "Understanding the origins of bias in word embeddings." arXiv preprint arXiv:1810.03611 (2018).	

Caliskan, Aylin, Joanna J. Bryson, and Arvind Narayanan. "Semantics derived automatically from language corpora contain human-like biases." Science 356, no. 6334 (2017): 183-186.

Garg, Nikhil, Londa Schiebinger, Dan Jurafsky, and James Zou. "Word embeddings quantify 100 years of gender and ethnic stereotypes." Proceedings of the National Academy of Sciences 115, no. 16 (2018): E3635-E3644.

Wevers, Melvin. "Using Word Embeddings to Examine Gender Bias in Dutch Newspapers, 1950-1990." arXiv preprint arXiv:1907.08922 (2019).

