**[onder constructie, niet publiceren]**
## Inleiding: hoe kranten 'mannen' en 'vrouwen' verbeelden

"Waarom meisjes glimlachen en jongens niet huilen" onderzoekt genderstereotypering in Nederlandse historische kranten. Het project maakt gebruik van computationele technieken om de stereotype weergave van "mannen" en "vrouwen" in de 19e en 20e eeuw te analyseren.

Om duidelijk te maken wat we met "stereotypen" of "vooroordelen" bedoelen, laten we een fragment genomen uit het "Nieuwsblad van het Noorden" bekijken. Zoals andere tijdschriften, poogde het Niewsblad hun lezers te onderwijzen door morele of filosofische spreekwoorden te verspreiden. Op maandag 9 maart 1896 staat er “Door zachtheid regeert de vrouw, door kracht de man." Dergelijke overpeinzingen over het "correcte" gedrag van vrouwen en mannen waren wijdverbreid. Het fragmet keert vaak terug, zij het met kleine variaties: "Eene der schoonste deugden van de vrouw is zachtheid" schrijft de Rotterdamse Courant in 1896. Enkele jaren later, in 1912, resoneert deze zin in een passage gepubliceerd in Turbantie: "De meest gewenschte eigenschap de vrouw is zachtheid." Vrouwen afbeelden als "zacht" en mannen als "krachtig" komt neer op een flagrante vorm van stereotypering, waarmee we een discursief proces bedoelen waarbij eigenschappen selectief worden toegeschreven aan sociale categorieën. 

Een eenvoudige zoekopdracht in de Delpher Krantendatabase over "vrouw" en "zachtheid" levert meer dan 12.000 hits op--en suggereert dus dat zulke ideeën grotendeels gemeengoed waren, zeker tot aan het midden van de twintigste eeuw. Hoewel dergelijke zoekopdracht nuttige inzichten opleveren, blijft het moeilijk om de (1) de verspreiding van dergelijke stereotypen te kwantificeren en (2) de sterkte waarmee gender-categorieën werden geassocieerd met bepaalde attributen (bijv. wrouwen met "zachtheid" en mannen met "kracht"). Traditionele historische methoden maakten gebruik van dichtbij lezen om deze vraag te beantwoorden, maar een dergelijke benadering schaalt niet naar een corpus zo groot als de Delpher-database. Dit project streeft daarom naar een computationele "lezen op afstand" benadering van gendervooroordeel in kranten; het beoogt zowel de inhoud als de verspreiding van genderstereotypen in Nederlandse kranten te meten.

## Onderzoekscontext: _bias_ en kunstmatige intelligentie

Het project past in een grotere onderzoeksagenda gericht op _bias_* in kunstmatige intelligentie (AI). Stel dat AI in opkomst is als een doordringende technologie - die vele aspecten van het leven doordringt, van online winkelen tot verkiezingscampagnes - onderzoekers maken zich steeds meer zorgen over de sociale en politieke effecten ervan. Specifieke toepassingen, zoals gezichtsherkenning, zijn bekritiseerd omdat ze mensen van kleur discrimineren. De bestaande systemen werken goed voor het herkennen van gezichten van sociaal-economische machtige groepen (zoals blanke mannen) maar kunnen die met andere [huidtinten en gelaatsstructuur](https://www.ted.com/talks/joy_buolamwini_how_i_m_fighting_bias_in_algorithms?language=en) niet nauwkeurig lezen .

Het risico is dat AI offline ongelijkheden naar het online domein doorvoert. Hetzelfde risico geldt voor de reproductie van sociale vooroordelen. AI-applicaties zijn vaak gebaseerd op "Machine Learning", een techniek waarbij computers een bepaald gedrag "leren" van een aantal voorbeelden (denk aan het spamfilter, dat leert om inkomende e-mail als spam te herkennen of niet, gegeven een aantal voorbeelden) . De materialen waaruit machines leren, worden problematisch geproduceerd door mensen, en veel artikelen hebben recent aangetoond hoe de machines tijdens het leren ook de vooroordelen en vooroordelen verwerven van degenen die de gegevens hebben geproduceerd.

Om dit meer in detail uit te leggen, gaan we naar het voorbeeld van Wbed's (Word embeddings-modellen), een verzameling algoritmen die associaties leren tussen woorden op basis van co-voorkomen in teksten. Een populair model uit 2013 is Word2Vec, dat in feite een semantische ruimte creëert (denk aan een kubus als een driedimensionale vectorruimte, gedefinieerd door x-, y- en z-assen), en woorden duwt die vaak voorkomen (binnen een bepaalde context) dicht bij elkaar, terwijl anderen worden gescheiden. Ik ben hier aan het bezuinigen, voor een goede introductie van Word2Vec voor Digital Humanities lees dit [blogbericht](http://bookworm.benschmidt.org/posts/2015-10-25-Word-Embeddings.html) van Ben Schmidt .

De basislijn hier is dat Word2Vec een constellatie van woorden in een vectorruimte genereert (dat wil zeggen dat het woorden in een spatie "insluit"), waarmee onderzoekers vervolgens kunnen berekenen hoe vergelijkbaar twee woorden zijn ("kat" is dichter bij "hond" dan bij "steen", aangezien de eerste veel contexten delen in tegenstelling tot de laatste).

Word2Vec is een krachtig hulpmiddel voor veel AI-toepassingen (kijk naar het aantal citaten van Google Scholar om jezelf te overtuigen). Seminal werk van (Bolukbasi et al., 2016) toonde echter aan dat het de neiging heeft om "overduidelijk seksistische" inbeddingen te produceren, waarin bijvoorbeeld vrouwen worden geassocieerd met beroep als "verpleegkundige" en "receptionisten" en mannen met "meastro" of "filosoof".

Deze bevinding genereerde een stroom artikelen die sociale voorkeur in modellen voor het insluiten van woorden verkennen. Terwijl (Bolukbasi et al., 2016) zich concentreerden op 'debiasing' inbedding (om verspreiding van de stereotypen naar andere AI-frameworks te voorkomen), gebruikten andere artikelen bias een sociaal signaal, als een lens op de maatschappij en geschiedenis. (Caliskan et al., 2017) demonstreerden rigoureus het bestaan ​​van mensachtige vooroordelen in WEM's, door stereotypen afgeleid van WEM's te vergelijken met die verkregen uit geaccepteerde psychologische tests (zoals de Implicit Association Test).

## Verwachte uitkomsten: bias als historisch signaal

Dit project breidt deze inzichten uit tot het domein van de geschiedenis, ervan uitgaande dat als woordinbedding getrainde hedendaagse gegevens hedendaagse vooroordelen repliceren, hetzelfde geldt voor het verleden.

(Garg et al., 2018) zorgden voor een meer rigoureuze validatie van deze intuïtie met het argument dat woordinbedding getraind op historische gegevens de veranderende verdeling van mannen en vrouwen in het personeel weerspiegelt, maar ook stereotiepe eigenschappen vastlegt. Door het bestuderen van de bijvoeglijke naamwoorden geassocieerd met Chinese namen, hebben de auteurs aangetoond dat de houding ten opzichte van Aziatische Amerikanen evolueerde van negatief (Chinees wordt gezien als "barbaars") naar grotendeels positief (Aziatische Amerikanen als de "model" minderheid). Met andere woorden, woordbedding stelt ons in staat om vertekening in de tijd te volgen. Hoewel we niet terug kunnen gaan naar het verleden, en vorige generaties kunnen onderwerpen aan psychologische tests (om de stereotypen die ze koesteren te achterhalen), lijken de resultaten van WEM's sterk op die welke zouden zijn verkregen met behulp van een tijdmachine.

In navolging van de krant (Garg et al. 2018) - en meer recentelijk (Wevers, 2019) - gebruiken we historische kranten om de gendervooroordelen van de afgelopen generaties te onderzoeken. Maar op welke manier zal dit onderzoek (en de verwachte resultaten) de huidige stand van de techniek verbeteren en andere onderzoeken helpen? Over het algemeen hopen we bij te dragen aan een meer verfijnd begrip van gendervooroordeel en de verspreiding ervan in Nederland.

- Ten eerste is het doel om een ​​eenvoudig hulpmiddel te maken waarmee onderzoekers de training van woordinbedding kunnen parametriseren, b.v. selecteer artikelen per jaar en specifieke krantenattributen (zoals bereik (nationaal of provinciaal), plaats van publicatie, politieke voorkeur en / of religieuze oriëntatie). Na het selecteren van de trainingsgegevens, kan de gebruiker kiezen tussen Word2Vec (2013) of het recentere FastText (2016) -algoritme om de woordinbedding te genereren. We zijn van plan te experimenteren met het verfijnen van contextuele modellen (zoals Bert). Alle modellen die zijn getraind als onderdeel van het onderzoek, zullen openbaar worden gemaakt.

- Ten tweede maken we het begrip "mannen" en "vrouwen" ingewikkelder: bestaande kranten gebruiken een relatief kleine (of bepaalde) reeks woorden om gendervooroordelen te berekenen. Ze berekenden hoe bepaalde attributen (bijv. "Vriendelijkheid") worden geassocieerd met een vrouwelijke / mannelijke naam of expliciet gesekse woorden zoals het voornaamwoord "zij". Voor een goed historisch onderzoek zijn we echter van plan eerst de hedendaagse terminologie (welke woorden door de historische krant werden gebruikt om mannen en vrouwen te beschrijven) onder de loep te nemen en te bekijken of de vertekening van toepassing was op alle mannelijke of vrouwelijke woorden, of het resultaat is van bijzonder sterk associaties met een bepaalde subcategorie (bijvoorbeeld moeders of meer bepaalde beroepen).

- Ten derde willen we, naast het monitoren van de verandering in bias op macroschaal (dat wil zeggen per decennium), begrijpen waar de bias eigenlijk vandaan komt (Brunet et al., 2019), dwz welke artikelen, kranten, acteurs of debatten zijn toegenomen op verminderde de bias? In die zin heeft ons onderzoek een fijnmazig begrip van de circulatie van vertekening in tijd en ruimte.

* We laten de term _bias_ onvertaald onwille van haar complexiteit.

## References

Bolukbasi, Tolga, Kai-Wei Chang, James Y. Zou, Venkatesh Saligrama, and Adam T. Kalai. "Man is to computer programmer as woman is to homemaker? debiasing word embeddings." In Advances in neural information processing systems, pp. 4349-4357. 2016.

Brunet, Marc-Etienne, Colleen Alkalay-Houlihan, Ashton Anderson, and Richard Zemel. "Understanding the origins of bias in word embeddings." arXiv preprint arXiv:1810.03611 (2018).	

Caliskan, Aylin, Joanna J. Bryson, and Arvind Narayanan. "Semantics derived automatically from language corpora contain human-like biases." Science 356, no. 6334 (2017): 183-186.

Garg, Nikhil, Londa Schiebinger, Dan Jurafsky, and James Zou. "Word embeddings quantify 100 years of gender and ethnic stereotypes." Proceedings of the National Academy of Sciences 115, no. 16 (2018): E3635-E3644.

Wevers, Melvin. "Using Word Embeddings to Examine Gender Bias in Dutch Newspapers, 1950-1990." arXiv preprint arXiv:1907.08922 (2019).

