## Inleiding: hoe kranten 'mannen' en 'vrouwen' construeren

"Waarom meisjes glimlachen en jongens niet huilen", onderzoekt hoe Nederlandse kranten gendernormen vormden. Het maakt gebruik van computationele technieken om stereotypen te detecteren in de weergave van "mannen" en "vrouwen" in de 19e en 20e eeuw.

Laten we, om een ​​voorbeeld van dergelijke stereotypering te geven, een citaat uit het "Nieuwsblad van het Noorden" bekijken, dat zoveel andere tijdschriften als doel hadden hun lezers op te leiden door morele of filosofische spreekwoorden te drukken. Op maandag 9 maart 1896 staat er “Door zachtheid
regeert de vrouw, door kracht de man. ”Zulke overpeinzingen over de" juiste "genderrollen waren
wijdverbreid in het Nederlandse medialandschap, vormden ze een culturele trope, vaak herhaald, zij het met kleine variaties: “Eene der schoonste deugden van de vrouw
is zachtheid 'schrijft de' Rotterdamse Courant 'in 1896. Enkele jaren later, in 1912, resoneert de zin in Turbantie:' De meest gewenschte eigenschap de vrouw is zachtheid '.

Het aantal voorbeelden zou ad nauseam uitgebreid kunnen worden: een eenvoudige zoekopdracht in de Delpher Krantendatabase over "vrouw" en "zachtheid" levert meer dan 12.000 hits op. Vrouwen afbeelden als 'zacht' en mannen als 'krachtig' komt neer op een flagrante vorm van stereotypering: een proces waarbij eigenschappen selectief worden toegeschreven aan sociale categorieën.

Hoewel dergelijke vragen nuttige inzichten opleveren in de attributen die aan vrouwen en mannen worden toegekend, blijft het moeilijk om de verspreiding van dergelijke stereotypen en de sterkte waarmee "vrouwen" werden geassocieerd met "zachtheid" en mannen met "macht" te kwantificeren. Traditionele, "kritische" methoden gebruikten nauwgezet lezen om deze vraag te beantwoorden, maar een dergelijke benadering is niet schaalbaar tot een corpus zo groot als de Delpher-database. Dit project streeft daarom naar een computationele "lezen op afstand" benadering van gendervooroordeel in kranten, met als doel zowel de inhoud als de verspreiding van genderstereotypen in Nederlandse kranten te meten.

## Onderzoekscontext: bias en kunstmatige intelligentie

Het project sluit aan bij een breder onderzoek naar agenda over bias in kunstmatige intelligentie. Omdat AI een doordringende technologie wordt en vele aspecten van het leven doordringt - van online winkelen tot het vormen van een politieke mening - zijn onderzoekers zich steeds meer zorgen gaan maken over de sociale en politieke effecten ervan. Specifieke technologieën zoals gezichtsherkenning zijn bekritiseerd voor het discrimineren van mensen van kleur. De bestaande systemen werken goed voor het herkennen van gezichten van sociaal-economische machtige groepen (zoals blanke mannen) maar kunnen die zwarte vrouwen niet nauwkeurig lezen.

Het risico hier is dat offline ongelijkheden online worden gereproduceerd door het gebruik van AI. Hetzelfde geldt voor vooroordelen. AI-applicaties zijn vaak gebaseerd op "Machine Learning", een techniek waarbij computers een bepaald gedrag "leren" van een set van voorbeelden (denk aan de spamfilter, die een inkomende e-mail leert te herkennen als spam of niet, gegeven een set van voorbeelden ). Problematisch, de materialen waarvan machines leren, worden geproduceerd door mensen, en veel artikelen hebben recent aangetoond hoe de machines tijdens het leren de vooroordelen en vooroordelen verwerven van degenen die de gegevens produceerden

Om dit meer in detail uit te leggen, gaan we naar het voorbeeld van taalmodellering.
Dit laatste klinkt moeilijk, maar komt neer op een vrij eenvoudige taak, namelijk het raden van de ontbrekende (of gemaskerde) elementen in een reeks woorden.
Deze techniek wordt gebruikt om een ​​computer te leren wat woorden "betekenen": als een taalmodel nauwkeurig kan voorspellen dat "melk" (of "water") zijn,
Wat er op de achtergrond gebeurt, is dat algoritmen associaties leren tussen woorden op basis van het samen voorkomen in teksten. Een populair model uit 2013 is Word2Vec. Kortom, Word2Vec maakt een semantische ruimte (denk aan een kubus als een driedimensionale vectorruimte, gedefinieerd door x-, y- en z-assen), en duwt woorden die de neiging hebben om (binnen een bepaalde context) samen te komen dicht bij elkaar. Ik ben hier aan het bezuinigen, lees deze [website] () voor een juiste introductie tot Word2Vec.
De basislijn hier is dat Word2Vec een volgorde van woorden genereert in een vectorruimte (in het algemeen een woordinsluitingsmodel genoemd) volgens hun co-voorkomen, waarmee onderzoekers vervolgens kunnen berekenen hoe vergelijkbaar twee items (bijvoorbeeld kat en hond dichter bij elkaar staan ​​dan kat) en steen, zoals eerstgenoemde veel contexten deelt, maar laatstgenoemde niet).

Word2Vec is een krachtig hulpmiddel voor veel toepassingen (kijk naar het aantal citaten van Google Scholar om jezelf te overtuigen). Het baanbrekende werk van (Bolukbasi et al., 2016) toonde echter aan dat het de 'flagrant seksistische' inbedding opleverde, waarin bijvoorbeeld vrouwen worden geassocieerd met beroep als 'verpleegkundige' en 'receptionisten' en mannen met 'meastro' of "filosoof". Deze bevinding genereerde een stroom artikelen die sociale voorkeur in modellen voor het insluiten van woorden verkennen. Terwijl (Bolukbasi et al., 2016) zich concentreerden op 'debiasing' inbedding (om verspreiding van de stereotypen naar andere AI-frameworks te voorkomen), gebruikten andere artikelen bias een sociaal signaal, als een lens op de maatschappij en geschiedenis. (Caliskan et al., 2017) heeft systematisch het bestaan ​​van mensachtige vooroordelen in WEM's aangetoond, door stereotypen afgeleid van WEM's te vergelijken met die verkregen uit geaccepteerde psychologische tests (zoals de Implicit Association Test). Ze bestreken een uiteenlopende reeks vooroordelen, van de nogal onschuldige (bloemen zijn "aangenamer" dan insecten) tot de meer giftige (de hardnekkige associatie van vrouwelijke namen met familie- en mannelijke namen met carrière).

Verwachte uitgangen: bias als historisch signaal

Dit project breidt deze inzichten uit tot het domein van de geschiedenis, in de veronderstelling dat als woordinbedding van getrainde hedendaagse teksten hedendaagse vooroordelen repliceren, hetzelfde geldt voor gegevens uit het verleden.

(Garg et al., 2018) zorgden voor een meer rigoureuze validatie van deze intuïtie en hebben aangetoond dat woordinbeddingen getraind op historische gegevens de veranderende verdeling van mannen en vrouwen in het personeelsbestand weerspiegelen (dwz de mate waarin een 'secretaresse' of 'timmerman' 'Is waarschijnlijk' mannelijk 'of' vrouwelijk '), maar legt ook stereotiepe eigenschappen vast. Door het bestuderen van de bijvoeglijke naamwoorden geassocieerd met Chinese namen, hebben de auteurs aangetoond dat de houding ten opzichte van Aziatische Amerikanen evolueerde van negatief (Chinees wordt gezien als "barbaars") naar grotendeels positief (Aziatische Amerikanen als de "model" minderheid). Met andere woorden, woordbedding weerspiegelt de vorming van in-groep / out-groep bias in de tijd. Hoewel we niet terug kunnen gaan naar het verleden, en vorige generaties kunnen onderwerpen aan psychologische tests (om de stereotypen te achterhalen die zij hebben
koesteren), lijken de resultaten van WEM's sterk op die welke dat zouden doen
zijn verkregen met behulp van een tijdmachine.

In navolging van de kranten (Garg et al. 2018), en recentelijk (Wevers, 2019), gebruiken we historische kranten om de gendervooroordelen van de afgelopen generaties te onderzoeken. Maar op welke manier zal dit onderzoek (en de verwachte resultaten) de huidige stand van de techniek verbeteren en ander onderzoek helpen. Over het algemeen hopen we dat ons onderzoek bijdraagt ​​aan een beter begrip van gendervooroordeel en de verspreiding ervan in Nederland.

- Ten eerste is het doel om een ​​eenvoudige tool te maken waarmee de onderzoeker de taalmodellen van de training kan parametreren, b.v. selecteer artikelen per jaar en de kenmerken van de krant (zoals bereik (nationaal of provinciaal), plaats van publicatie, politieke voorkeur en / of religieuze oriëntatie). Na het selecteren van trainingsgegevens kan de gebruiker kiezen tussen Word2Vec (2013) of het recentere FastText (2016) -algoritme om de woordinbedding te genereren. We zijn van plan te experimenteren met het verfijnen van contextuele modellen (zoals Elmo en Bert). Alle modellen die zijn getraind als onderdeel van het onderzoek, zullen openbaar worden gemaakt.

- Ten tweede maken we het begrip "mannen" en "vrouwen" ingewikkelder: bestaande kranten gebruiken een relatief kleine reeks woorden om gendervooroordeel te berekenen. Ze berekenen hoe bepaalde attributen (bijv. "Vriendelijkheid") worden geassocieerd met een vrouwelijke / mannelijke naam of expliciet gesekse woorden zoals "zij"). Voor een goed historisch onderzoek zijn we echter van plan eerst de hedendaagse terminologie (welke woorden door de historische krant werden gebruikt om mannen en vrouwen te beschrijven) onder de loep te nemen en te bekijken of de vertekening van toepassing was op alle mannelijke of vrouwelijke woorden, of het resultaat is van bijzonder sterk associaties met een bepaalde subcategorie (bijvoorbeeld moeders of meer bepaalde beroepen).

- Ten derde willen we, naast het monitoren van de verandering in bias, begrijpen waar het vandaan komt (Brunet et al., 2019), d.w.z. welke artikelen, kranten, acteurs of debatten zijn toegenomen door de bias te verminderen? In die zin heeft ons onderzoek een fijnmazig begrip van de circulatie van vertekening in tijd en ruimte.