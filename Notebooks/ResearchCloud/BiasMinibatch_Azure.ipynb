{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias by Minibatch \n",
    "\n",
    "Notebook for analyzing the bias per minibatch of documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "from gensim.models.word2vec import Word2Vec \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook\n",
    "from scipy.spatial.distance import cosine\n",
    "from glob import glob\n",
    "import pickle\n",
    "from utils_parallel import *\n",
    "#import logging\n",
    "#logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "START_YEAR = 1860\n",
    "END_YEAR = 1870\n",
    "ROOT = \"/home/kaspar/ResearchDrive\"\n",
    "MODEL_PATH = \"/home/kaspar/models/{}-{}.w2v.model\".format(START_YEAR,END_YEAR)\n",
    "OUTPUT = \"/home/kaspar/processed\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect sentences\n",
    "\n",
    "Collect the sentences that will be merged into minibatches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = SentIterator(ROOT,date_range=(START_YEAR,END_YEAR),processed_path='/home/kaspar/processed',tokenized=False,n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_sents_path = sentences.filter_lines('(?:vrouw*|moeder*)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_sents_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_sents_lines = open('/home/kaspar/processed/1860-1870-_filtered.txt','r').read().split('\\n\\n')\n",
    "sent_df = pd.DataFrame([s.split('<SEP>') for s in filtered_sents_lines],columns=['doc_id','text'])\n",
    "sent_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect Identifiers\n",
    "\n",
    "Here we read the Identifier csv files with metadata on each article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csvs = glob('/home/kaspar/Identifiers/Identifiers_18*.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([pd.read_csv(f,sep=';',index_col=0) for f in csvs],axis=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doc_id(identifier):\n",
    "    \"\"\"reconstruct id from row in Identifier files\n",
    "    these ids match one reported in the xml.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        _,_,i,_,j,_ = identifier.split(\":\") \n",
    "        return '_'.join([i,j])\n",
    "    except:\n",
    "        return 'NaN'\n",
    "    \n",
    "df['doc_id'] = df.identifier.apply(get_doc_id)\n",
    "print(np.sum(df.doc_id=='NaN'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge sentences with metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = sent_df.merge(df,how='left',right_on='doc_id',left_on='doc_id')\n",
    "print(df_merged.shape)\n",
    "df_merged.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we group the article by day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_articles = df_merged.groupby('date')['text'].apply('\\n'.join)\n",
    "daily_articles = pd.DataFrame(daily_articles,columns=['text'])\n",
    "daily_articles['doc_id'] = ''\n",
    "daily_articles.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_articles.to_csv('../../../processed/{}_{}-daily.csv'.format(START_YEAR,END_YEAR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Bias by mini-batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_articles = pd.read_csv('../../../processed/{}_{}-daily.csv'.format(START_YEAR,END_YEAR),chunksize=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "model = Word2Vec.load(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for similation we now use the nearest neighbours as the lexicon for male and female words\n",
    "p1 = [w for w,v in model.wv.most_similar('vrouw',topn=20)] + ['vrouw']\n",
    "p2 = [w for w,v in model.wv.most_similar('man',topn=20)] + ['man']\n",
    "# target is the word child\n",
    "target = [w for w,v in model.wv.most_similar('kind',topn=20)] + ['kind']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_sim = lambda v1,v2: 1 - cosine(v1,v2) \n",
    "euclid_dist = lambda v1,v2: - np.linalg.norm(v1-v2,ord=2)\n",
    "average_vector = lambda words,model : np.mean([model.wv.__getitem__(w) for w in words if model.wv.__contains__(w)],axis=0)\n",
    "\n",
    "def compute_bias(p1,p2,target,model,metric=cosine_sim):\n",
    "    \"\"\"computes bias given two poles and and a target word list\n",
    "    bias is the average distance of each target word to the poles\n",
    "    Arguments:\n",
    "        p1 (list): list of pole words\n",
    "        p2 (list): lost of pole words\n",
    "        target (list): list of target words\n",
    "        metric (funtion): distance function, either cosine or euclidean\n",
    "    Returns:\n",
    "        bias (float): the bias score of the target to each of the poles\n",
    "    \"\"\"\n",
    "    av_v1 = average_vector(p1,model); av_v2 = average_vector(p2,model)\n",
    "    return np.mean([metric(av_v1,model.wv.__getitem__(w)) - \\\n",
    "                      metric(av_v2,model.wv.__getitem__(w)) for w in target \n",
    "                           if w in model.wv])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update model and compute bias\n",
    "Update model with new sentences and compute the bias scores over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_sents = (preprocess_sent(t.text,t.doc_id)\n",
    "                    for chunk in daily_articles\n",
    "                        for i,t in chunk.iterrows())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters for training\n",
    "EPOCH = 4\n",
    "# Important: add learning rate!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_bias(i,sent,p1,p2,target,model_path='../../../models/{0}-{1}.w2v.model'.format(START_YEAR,END_YEAR)):\n",
    "    \"\"\"function that compares the bias scores before and after updating the model weights.\n",
    "    Arguments:\n",
    "        i (int): row index # to do: improve here\n",
    "        sent (list): list of strings that contains the document on which to retrain the model\n",
    "        p1 (list): list of pole words\n",
    "        p2 (list): lost of pole words\n",
    "        target (list): list of target words\n",
    "    Returns:\n",
    "        a tuple with i, sent and difference in bias caused by updating the model\n",
    "        \n",
    "    \"\"\"\n",
    "    model = Word2Vec.load(model_path)\n",
    "    model.train([sent],total_examples=len([sent]),epochs=EPOCH)\n",
    "    orig_model = Word2Vec.load(model_path)\n",
    "    return (i,sent,compute_bias(p1,p2,target,model) - compute_bias(p1,p2,target,orig_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1860-1870.w2v.model\t\t\t    1860-1870.w2v.model.wv.vectors.npy\r\n",
      "1860-1870.w2v.model.trainables.syn1neg.npy\r\n"
     ]
    }
   ],
   "source": [
    "!ls ../../../models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ce951d4ebba4c7db6bedd260a070e76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# compute the bias scores of all sentences\n",
    "scores = Parallel(n_jobs=8)(delayed(compare_bias)(i,sent,p1,p2,target) for i,sent in tqdm_notebook(enumerate(update_sents)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "649"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "for i,sent in tqdm_notebook(enumerate(update_sents)):\n",
    "    scores.append(compare_bias(i,sent,p1,p2,target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('{}/biasbatch.pckl'.format(OUTPUT),'wb') as out_pickle:\n",
    "    pickle.dump(scores,out_pickle)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
